{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP ECI 2019 \n",
    "\n",
    "## Trabajo Práctico\n",
    "\n",
    "### Introducción\n",
    "\n",
    "El objetivo del trabajo práctico es reproducir el resultado obtenido en [Gururangan et al., 2018](https://www.aclweb.org/anthology/N18-2017) en el área del _Natural Language Inference_ obtenido sobre el dataset [SNLI](https://nlp.stanford.edu/projects/snli/). \n",
    "En esta tarea se debe responder, dadas dos frases A y B, si B es implicación de A (\"entailment\"), B es contradictorio con A (\"contradiction\") o si lo que enuncia B es neutral respecto de A (\"neutral\"). Se dice que A es la premisa y B es la hipótesis. En este trabajo práctico intentaremos predecir a qué clase pertenece cada una de las hipótesis sin observar la premisa.\n",
    "\n",
    "### Desarrollo\n",
    "\n",
    "Para replicar los resultados obtenidos en [Gururangan et al., 2018](https://www.aclweb.org/anthology/N18-2017) haremos uso de un clasificador lineal generalizado implementado en la librería [FastText](https://fasttext.cc/). El mismo comienza por construir los _embeddings_ correspondientes a las palabras del corpus suministrado mediante un algoritmo similar a CBOW, para posteriormente alimentar un clasificador lineal en su entrenamiento con el objetivo de predecir las etiquetas correctas (`entailment`, `neutral` o `contradiction` en nuestro caso). Una descripción completa de la técnica utilizada puede hallarse en [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759), mientras que las referencias de la API de este clasificador pueden hallarse en [train_supervised fasttext python API](https://fasttext.cc/docs/en/python-module.html#train_supervised-parameters)\n",
    "\n",
    "Comenzaremos por implementar nuestro clasificador _custom_, usando a fasttext como backend, para facilitar las posteriores tareas de seleccion de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class FastTextClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Base classifier for fasttext.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        storage_path='data',\n",
    "        wordNgrams=2,\n",
    "        ws=5,\n",
    "        lr=0.1,\n",
    "        dim=90,\n",
    "        epoch=100,\n",
    "        minCount=2,\n",
    "        verbose=1):\n",
    "        \"\"\"Set instance parameters.\"\"\"\n",
    "        self.storage_path = storage_path\n",
    "        self.wordNgrams = wordNgrams\n",
    "        self.ws = ws\n",
    "        self.lr = lr\n",
    "        self.dim = dim\n",
    "        self.epoch = epoch\n",
    "        self.minCount = minCount\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def get_dev_and_train(\n",
    "            self,\n",
    "            train_data='snli_1.0_train_filtered.jsonl',\n",
    "            train_labels='snli_1.0_train_gold_labels.csv',\n",
    "            dev_data='snli_1.0_dev_filtered.jsonl',\n",
    "            dev_labels='snli_1.0_dev_gold_labels.csv',\n",
    "            key='pairID'):\n",
    "        \"\"\"Preprocess train and dev data.\"\"\"\n",
    "        train_data_path = os.path.join(\n",
    "            self.storage_path, train_data)\n",
    "        train_labels_path = os.path.join(\n",
    "            self.storage_path, train_labels)\n",
    "        dev_data_path = os.path.join(\n",
    "            self.storage_path, dev_data)\n",
    "        dev_labels_path = os.path.join(\n",
    "            self.storage_path, dev_labels)\n",
    "\n",
    "        train_info = pd.read_json(train_data_path, lines=True)\n",
    "        dev_info = pd.read_json(dev_data_path, lines=True)\n",
    "\n",
    "        train_labels = pd.read_csv(train_labels_path)\n",
    "        dev_labels = pd.read_csv(dev_labels_path)\n",
    "\n",
    "        train = train_info.set_index(key).join(train_labels.set_index(key))\n",
    "        dev = dev_info.set_index(key).join(dev_labels.set_index(key))\n",
    "        train = train[['sentence2', 'gold_label']]\n",
    "        dev = dev[['sentence2', 'gold_label']]\n",
    "\n",
    "        df = train.append(dev)\n",
    "\n",
    "        return df, train, dev\n",
    "\n",
    "    def preprocess(self, X, lower=True, replace_dict={}):\n",
    "        X_new = X.copy()\n",
    "        if lower:\n",
    "            X_new = X_new.str.lower()\n",
    "        for k, v in replace_dict.items():\n",
    "            X_new = X_new.str.replace(k, v)\n",
    "        return X_new\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        buffer_filename = os.path.join(self.storage_path, 'buffer.csv')\n",
    "        series = X + ' __label__' + y\n",
    "        series.to_csv(buffer_filename, encoding='utf-8', index=False, header=False)\n",
    "        logging.info('Fitting classifier...')\n",
    "        model = fasttext.train_supervised(\n",
    "            buffer_filename,\n",
    "            wordNgrams=self.wordNgrams,\n",
    "            ws=self.ws,\n",
    "            lr=self.lr,\n",
    "            dim=self.dim,\n",
    "            epoch=self.epoch,\n",
    "            minCount=self.minCount,\n",
    "            verbose=self.verbose)\n",
    "\n",
    "        self.model_ = model\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y = []\n",
    "        try:\n",
    "            getattr(self, \"model_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "\n",
    "        for sentence in X:\n",
    "            y.append(\n",
    "                self.model_.predict(sentence)[0][0].replace('__label__', ''))\n",
    "        return y\n",
    "\n",
    "    def score(self, X, y, metric='accuracy', sample_weight=None):\n",
    "        y_hat = self.predict(X)\n",
    "        if metric == 'accuracy':\n",
    "            return accuracy_score(y, y_hat, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El clasificador se inicia con algunos parámetros fijos y se provee un diccionario para pre-procesar el corpus. Los pasos de este pre-procesamiento serán\n",
    "\n",
    "- Convertir todas las palabras a minúscula\n",
    "- Eliminar todos los caracteres no alfanuméricos\n",
    "- Remplazar a los números escritos por sus dígitos\n",
    "\n",
    "El dataset utilizado para el entrenamiento será el provisto para `train` y será posteriormente validado con `dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftc = FastTextClassifier()\n",
    "\n",
    "replace_dict = {\n",
    "    '[^a-z1-9 ]+': '',\n",
    "    'one': '1',\n",
    "    'two': '2',\n",
    "    'three': '3',\n",
    "    'four': '4',\n",
    "    'five': '5',\n",
    "    'six': '6',\n",
    "    'seven': '7',\n",
    "    'eight': '8',\n",
    "    'nine': '9'}\n",
    "\n",
    "total, train, dev = ftc.get_dev_and_train()\n",
    "train.sentence2 = ftc.preprocess(train.sentence2, replace_dict=replace_dict)\n",
    "dev.sentence2 = ftc.preprocess(dev.sentence2, replace_dict=replace_dict)\n",
    "total.sentence2 = ftc.preprocess(total.sentence2, replace_dict=replace_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos un _GridSearch_ para encontrar el mejor conjunto de parámetros para nuestro clasificador mediante _CrossValidation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n",
      "INFO:root:Fitting classifier...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=FastTextClassifier(dim=90, epoch=80, lr=0.1, minCount=2, storage_path='data',\n",
       "          verbose=1, wordNgrams=2, ws=5),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'wordNgrams': [2, 3], 'lr': [0.1, 0.25]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(FastTextClassifier(epoch=80) , param_grid=dict(\n",
    "    wordNgrams=[2, 3],\n",
    "    lr=[0.1, 0.25]))\n",
    "clf.fit(X=train.sentence2, y=train.gold_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y una vez obtenidos los parámetros los guardamos y analizamos el _score_ (_Accuracy_) alcanzado para el mejor de los modelos en el dataset de `dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model params {'dim': 90, 'epoch': 80, 'lr': 0.25, 'minCount': 2, 'storage_path': 'data', 'verbose': 1, 'wordNgrams': 2, 'ws': 5}\n",
      "Best model score: 0.6353767153833412 Best model score on dev: 0.6546433651696809\n"
     ]
    }
   ],
   "source": [
    "best_ftc = clf.best_estimator_\n",
    "print('Best model params', best_ftc.get_params())\n",
    "print('Best model score:', clf.best_score_,\n",
    "      'Best model score on dev:', best_ftc.score(X=dev.sentence2, y=dev.gold_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el mismo entrenamos nuestro clasificador ahora sobre el corpus completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fitting classifier...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FastTextClassifier(dim=90, epoch=80, lr=0.25, minCount=2, storage_path='data',\n",
       "          verbose=1, wordNgrams=2, ws=5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ftc.fit(X=total.sentence2, y=total.gold_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y realizamos la predicción final sobre el dataset de testeo. Guardamos el mismo para subirlo a la página de la competencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_json('data/snli_1.0_test_filtered.jsonl', lines=True)\n",
    "test_data.sentence2 = ftc.preprocess(test_data.sentence2, replace_dict=replace_dict)\n",
    "test_data['gold_label'] = best_ftc.predict(test_data.sentence2)\n",
    "results = test_data[['pairID', 'gold_label']]\n",
    "results.to_csv('best_estimator_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
